---
---

@string{aps = {American Physical Society,}}

@InProceedings{MGSV_ICCV2025,
    abbr={ICCV},
    title={Music Grounding by Short Video},
    author={Xin, Zijie and Wang, Minquan and Liu, Jingyu and Ma, Ye and Chen, Quan and Jiang, Peng and Li†, Xirong},
    booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
    year={2025},
    selected={true},
    abstract={Adding proper background music helps complete a short video to be shared. Previous work tackles the task by video-to-music retrieval (V2MR), aiming to find the most suitable music track from a collection to match the content of a given query video. In practice, however, music tracks are typically much longer than the query video, necessitating (manual) trimming of the retrieved music to a shorter segment that matches the video duration. In order to bridge the gap between the practical need for music moment localization and V2MR, we propose a new task termed Music Grounding by Short Video (MGSV). To tackle the new task, we introduce a new benchmark, MGSV-EC, which comprises a diverse set of 53k short videos associated with 35k different music moments from 4k unique music tracks. Furthermore, we develop a new baseline method, MaDe, which performs both video-to-music matching and music moment detection within a unified end-to-end deep network.  Extensive experiments on MGSV-EC not only highlight the challenging nature of MGSV but also set MaDe as a strong baseline.},
    paperlink={https://arxiv.org/abs/2408.16990},
    website={https://rucmm.github.io/MGSV},
    codelink={https://github.com/xxayt/MGSV},
    datalink={https://huggingface.co/datasets/xxayt/MGSV-EC},
    poster={https://iccv.thecvf.com/media/PosterPDFs/ICCV%202025/660.png},
    preview={MGSV.png},
}

@InProceedings{TeachCLIP_CVPR2024,
    abbr={CVPR},
    title={Holistic Features are almost Sufficient for Text-to-Video Retrieval},
    author={Tian*, Kaibin and Zhao*, Ruixiang and Xin, Zijie and Lan, Bangxiang and Li†, Xirong},
    booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    year={2024},
    selected={true},
    abstract={For text-to-video retrieval (T2VR), which aims to retrieve unlabeled videos by ad-hoc textual queries, CLIP-based methods currently lead the way. Compared to CLIP4Clip which is efficient and compact, state-of-the-art models tend to compute video-text similarity through fine-grained cross-modal feature interaction and matching, putting their scalability for large-scale T2VR applications into doubt. We propose TeachCLIP, enabling a CLIP4Clip based student network to learn from more advanced yet computationally intensive models. In order to create a learning channel to convey fine-grained cross-modal knowledge from a heavy model to the student, we add to CLIP4Clip a simple Attentional frame-Feature Aggregation (AFA) block, which by design adds no extra storage / computation overhead at the retrieval stage. Frame-text relevance scores calculated by the teacher network are used as soft labels to supervise the attentive weights produced by AFA. Extensive experiments on multiple public datasets justify the viability of the proposed method. TeachCLIP has the same efficiency and compactness as CLIP4Clip, yet has near-SOTA effectiveness.},
    paperlink={https://openaccess.thecvf.com/content/CVPR2024/html/Tian_Holistic_Features_are_almost_Sufficient_for_Text-to-Video_Retrieval_CVPR_2024_paper.html},
    codelink={https://github.com/ruc-aimc-lab/TeachCLIP},
    poster={https://cvpr.thecvf.com/media/PosterPDFs/CVPR%202024/29177.png},
    preview={TeachCLIP.png},
}

@InProceedings{LPD_MM2025,
    abbr={ACMMM},
    title={Learning Partially-Decorrelated Common Spaces for Ad-hoc Video Search},
    author={Hu, Fan and Xin, Zijie and Li†, Xirong},
    booktitle={Proceedings of the 33rd ACM international conference on Multimedia (ACMMM)},
    year={2025},
    selected={true},
    abstract={Ad-hoc Video Search (AVS) involves using a textual query to search for multiple relevant videos in a large collection of unlabeled short videos. The main challenge of AVS is the visual diversity of relevant videos. A simple query such as "Find shots of a man and a woman dancing together indoors" can span a multitude of environments, from brightly lit halls and shadowy bars to dance scenes in black-and-white animations. It is therefore essential to retrieve relevant videos as comprehensively as possible. Current solutions for the AVS task primarily fuse multiple features into one or more common spaces, yet overlook the need for diverse spaces. To fully exploit the expressive capability of individual features, we propose LPD, short for Learning Partially Decorrelated common spaces. LPD incorporates two key innovations: feature-specific common space construction and the de-correlation loss. Specifically, LPD learns a separate common space for each video and text feature, and employs de-correlation loss to diversify the ordering of negative samples across different spaces. To enhance the consistency of multi-space convergence, we designed an entropy-based fair multi-space triplet ranking loss. Extensive experiments on the TRECVID AVS benchmarks (2016-2023) justify the effectiveness of LPD. Moreover, diversity visualizations of LPD's spaces highlight its ability to enhance result diversity.},
    paperlink={https://arxiv.org/abs/2508.02340},
    codelink={https://github.com/xxayt/LPD},
    preview={LPD.png},
}

@InProceedings{MoSketch_ICCV2025,
    abbr={ICCV},
    title={Multi-Object Sketch Animation by Scene Decomposition and Motion Planning},
    author={Liu, Jingyu and Xin, Zijie and Fu, Yuhan and Zhao, Ruixiang and Lan, Bangxiang and Li†, Xirong},
    booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
    year={2025},
    selected={true},
    abstract={Sketch animation, which brings static sketches to life by generating dynamic video sequences, has found widespread applications in GIF design, cartoon production, and daily entertainment. While current sketch animation methods perform well in single-object sketch animation, they struggle in multi-object scenarios. By analyzing their failures, we summarize two challenges of transitioning from single-object to multi-object sketch animation: object-aware motion modeling and complex motion optimization. For multi-object sketch animation, we propose MoSketch based on iterative optimization through Score Distillation Sampling (SDS), without any other data for training. We propose four modules: LLM-based scene decomposition, LLM-based motion planning, motion refinement network and compositional SDS, to tackle the two challenges in a divide-and-conquer strategy. Extensive qualitative and quantitative experiments demonstrate the superiority of our method over existing sketch animation approaches. MoSketch takes a pioneering step towards multi-object sketch animation, opening new avenues for future research and applications.},
    paperlink={https://arxiv.org/abs/2503.19351},
    website={https://rucmm.github.io/MoSketch},
    codelink={https://github.com/jyliu-98/MoSketch},
    poster={https://iccv.thecvf.com/media/PosterPDFs/ICCV%202025/1396.png},
    preview={MoSketch.jpg},
}

@InProceedings{DAPL_ICME2025,
    abbr={ICME},
    title={DAPL: Integration of Positive and Negative Descriptions in Text-Based Person Search},
    author={Deng, Yuchuan and Hu, Zhanpeng and Xin, Zijie and Deng, Chuang and Zhao†, Qijun},
    booktitle={Proceedings of the IEEE International Conference on Multimedia and Expo (ICME)},
    year={2025},
    selected={true},
    abstract={Text-based person search (TBPS) aims to retrieve specific images of individuals from large datasets using textual descriptions. Existing TBPS methods focus primarily on identifying explicit positive attributes, often neglecting the critical role of negative descriptions. This oversight can lead to false positives, where images that should be excluded based on negative descriptions are incorrectly included, due to partial alignment with the positive criteria. To address this limitation, we propose the Dual Attribute Prompt Learning (DAPL) framework, which incorporates both positive and negative descriptions to improve the interpretative accuracy of vision-language models in TBPS tasks. DAPL combines Dual Image-Attribute Contrastive (DIAC) learning with Sensitive Image-Attribute Matching (SIAM) learning to enhance the detection of previously unseen attributes. Furthermore, to achieve a balance between coarse and finegrained alignment of visual and textual embeddings, we introduce the Dynamic Token-wise Similarity (DTS) loss. This loss function refines the representation of both matching and non-matching descriptions at the token level, providing more precise and adaptable similarity assessments, and ultimately improving the accuracy of the matching process. Empirical results demonstrate that DAPL outperforms state-of-the-art methods, enhancing both precision and robustness in TBPS tasks.},
    paperlink={https://ieeexplore.ieee.org/document/11210038},
    preview={DAPL.jpg},
}
