---
---

@string{aps = {American Physical Society,}}

@InProceedings{TeachCLIP_CVPR2024,
    abbr={CVPR},
    title={Holistic Features are almost Sufficient for Text-to-Video Retrieval},
    author={Tian*, Kaibin and Zhao*, Ruixiang and Xin, Zijie and Lan, Bangxiang and Li, Xirong},
    booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    year={2024},
    selected={true},
    abstract={For text-to-video retrieval (T2VR), which aims to retrieve unlabeled videos by ad-hoc textual queries, CLIP-based methods currently lead the way. Compared to CLIP4Clip which is efficient and compact, state-of-the-art models tend to compute video-text similarity through fine-grained cross-modal feature interaction and matching, putting their scalability for large-scale T2VR applications into doubt. We propose TeachCLIP, enabling a CLIP4Clip based student network to learn from more advanced yet computationally intensive models. In order to create a learning channel to convey fine-grained cross-modal knowledge from a heavy model to the student, we add to CLIP4Clip a simple Attentional frame-Feature Aggregation (AFA) block, which by design adds no extra storage / computation overhead at the retrieval stage. Frame-text relevance scores calculated by the teacher network are used as soft labels to supervise the attentive weights produced by AFA. Extensive experiments on multiple public datasets justify the viability of the proposed method. TeachCLIP has the same efficiency and compactness as CLIP4Clip, yet has near-SOTA effectiveness.},
    paperlink={https://openaccess.thecvf.com/content/CVPR2024/html/Tian_Holistic_Features_are_almost_Sufficient_for_Text-to-Video_Retrieval_CVPR_2024_paper.html},
    codelink={https://github.com/ruc-aimc-lab/TeachCLIP},
    poster={https://cvpr.thecvf.com/media/PosterPDFs/CVPR%202024/29177.png},
    preview={TeachCLIP.png},
}

@misa{VMMR_arxiv,
    abbr={arXiv},
    title={Music Grounding by Short Video},
    author={Xin, Zijie and Wang, Minquan and Jingyu, Liu and Chen, Quan and Ma, Ye and Jiang, Peng and Li, Xirong},
    year={2025},
    selected={true},
    abstract={Adding proper background music helps complete a short video to be shared. Towards automating the task, previous research focuses on video-to-music retrieval (VMR), aiming to find amidst a collection of music the one best matching the content of a given video. Since music tracks are typically much longer than short videos, meaning the returned music has to be cut to a shorter moment, there is a clear gap between the practical need and VMR. In order to bridge the gap, we propose in this paper video to music moment retrieval (VMMR) as a new task. To tackle the new task, we build a comprehensive dataset Ad-Moment which contains 50K short videos annotated with music moments and develop a two-stage approach. In particular, given a test video, the most similar music is retrieved from a given collection. Then, a Transformer based music moment localization is performed. We term this approach Retrieval and Localization (ReaL). Extensive experiments on real-world datasets verify the effectiveness of the proposed method for VMMR.},
    paperlink={xxx},
    codelink={https://github.com/xxayt/VMMR},
    project={https://rucmm.github.io/VMMR},
    preview={MGSV.png},
}