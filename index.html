<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>ZIJIE XIN 辛梓杰</title> <meta name="author" content="Zijie Xin"> <meta name="description" content="Welcome to Zijie Xin's personal website. "> <meta name="keywords" content="zijie xin, zijie, xin, 辛梓杰, 辛, 梓, 杰, jekyll, jekyll-theme, academic-website, portfolio-website"> <meta property="og:site_name" content="ZIJIE XIN 辛梓杰"> <meta property="og:type" content="website"> <meta property="og:title" content="ZIJIE XIN 辛梓杰 | ZIJIE XIN 辛梓杰"> <meta property="og:url" content="https://xxayt.github.io/"> <meta property="og:description" content="Welcome to Zijie Xin's personal website. "> <meta property="og:image" content="assets/img/yak2.png"> <meta property="og:locale" content="en"> <meta name="twitter:card" content="summary"> <meta name="twitter:title" content="ZIJIE XIN 辛梓杰"> <meta name="twitter:description" content="Welcome to Zijie Xin's personal website. "> <meta name="twitter:image" content="assets/img/yak2.png"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?7b22944fb09167ac56f8d613eed8ab64"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%8F%A0&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://xxayt.github.io/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?e74e74bf055e5729d44a7d031a5ca6a5" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script> <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">ZIJIE XIN 辛梓杰<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog</a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title"> ZIJIE XIN 辛梓杰 </h1> <p class="desc">PhD at <a href="https://www.ruc.edu.cn/" rel="external nofollow noopener" target="_blank">RUC</a>.</p> </header> <article> <div class="profile float-right"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/yak2-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/yak2-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/yak2-1400.webp"></source> <img src="/assets/img/yak2.jpg?bec2b05b8ed15a3f64c873cac7c08d25" class="img-fluid z-depth-1 rounded-circle" width="auto" height="auto" alt="yak2.jpg" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="more-info"> <p>Now Beijing</p> </div> </div> <div class="clearfix"> <p>I am an incoming PhD student at the <a href="https://ruc-aimc-lab.github.io/" rel="external nofollow noopener" target="_blank">AI &amp; Media Computing Lab</a>, <a href="https://www.ruc.edu.cn/" rel="external nofollow noopener" target="_blank">Renmin University of China</a>, advised by <a href="http://lixirong.net/" rel="external nofollow noopener" target="_blank">Prof. Xirong Li</a>. Previously, I received my Bachelor’s degree with honors in the Top-notch Program (a class of 15 elite students selected from 400+) from <a href="http://cs.scu.edu.cn/" rel="external nofollow noopener" target="_blank">College of Computer Science</a>, <a href="https://www.scu.edu.cn/" rel="external nofollow noopener" target="_blank">Sichuan University</a>, where I was advised by <a href="http://www.scubrl.org/qjzhao" rel="external nofollow noopener" target="_blank">Prof. Qijun Zhao</a>.</p> <p>My research primarily revolves around cross-media retrieval, open-set recognition, and computer vision, complemented by a broad curiosity in generative model and LLM.</p> </div> <h2><a href="/news/" style="color: inherit;">news</a></h2> <div class="news"> <div class="table-responsive" style="max-height: 60vw"> <table class="table table-sm table-borderless"> <tr> <th scope="row" style="width: 20%">Aug 15, 2024</th> <td> We have successfully submitted two papers to <a href="https://aaai.org/conference/aaai/aaai-25/" rel="external nofollow noopener" target="_blank">AAAI 2025</a>, with one of them as my first-author paper! Looking forward to good news! <img class="emoji" title=":sparkles:" alt=":sparkles:" src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png" height="20" width="20"> <img class="emoji" title=":smile:" alt=":smile:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f604.png" height="20" width="20"> </td> </tr> <tr> <th scope="row" style="width: 20%">Jun 28, 2024</th> <td> 🎉🎉I have successfully graduated with my bachelor’s degree from SCU and been honored as an Outstanding Graduate of Sichuan University! </td> </tr> <tr> <th scope="row" style="width: 20%">Apr 27, 2024</th> <td> I will serve as a reviewer for <a href="https://2024.acmmm.org/" rel="external nofollow noopener" target="_blank">ACM MM 2024</a>. </td> </tr> <tr> <th scope="row" style="width: 20%">Apr 13, 2024</th> <td> We have successfully submitted a paper to <a href="https://2024.acmmm.org/" rel="external nofollow noopener" target="_blank">ACM MM 2024</a> and are looking forward to good news! <img class="emoji" title=":sparkles:" alt=":sparkles:" src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png" height="20" width="20"> <img class="emoji" title=":smile:" alt=":smile:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f604.png" height="20" width="20"> </td> </tr> <tr> <th scope="row" style="width: 20%">Feb 27, 2024</th> <td> Our work about <a href="https://www.researchgate.net/publication/379270657_Holistic_Features_are_almost_Sufficient_for_Text-to-Video_Retrieval" rel="external nofollow noopener" target="_blank">a Multi-Grained Teaching Strategy for Efficient Text-to-Video Retrieval</a> was accepted at <a href="https://cvpr.thecvf.com/Conferences/2024" rel="external nofollow noopener" target="_blank">CVPR 2024</a>! 🎉 </td> </tr> <tr> <th scope="row" style="width: 20%">Nov 29, 2023</th> <td> 🎉🎉 I joined <a href="https://www.kuaishou.com/cn" rel="external nofollow noopener" target="_blank">Kuaishou</a> as a research internship on video-music retrieval. </td> </tr> <tr> <th scope="row" style="width: 20%">Nov 17, 2023</th> <td> We have successfully submitted a paper to <a href="https://cvpr.thecvf.com/Conferences/2024" rel="external nofollow noopener" target="_blank">CVPR 2024</a> and are looking forward to good news! <img class="emoji" title=":sparkles:" alt=":sparkles:" src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png" height="20" width="20"> <img class="emoji" title=":smile:" alt=":smile:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f604.png" height="20" width="20"> </td> </tr> <tr> <th scope="row" style="width: 20%">Oct 8, 2023</th> <td> After heading to Beijing and joining the <a href="https://ruc-aimc-lab.github.io/" rel="external nofollow noopener" target="_blank">AI &amp; Media Computing Lab</a>, I have unofficially started my PhD adventure! <img class="emoji" title=":nerd_face:" alt=":nerd_face:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f913.png" height="20" width="20"> </td> </tr> <tr> <th scope="row" style="width: 20%">Apr 20, 2023</th> <td> I joined <a href="https://gewu-lab.github.io/" rel="external nofollow noopener" target="_blank">GeWu-Lab</a> as a short-term intern. </td> </tr> </table> </div> </div> <h2><a href="/publications/" style="color: inherit;">Publications</a></h2> <div class="publications"> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr preview"> <abbr class="badge" style="background-color:#6820d0"><a href="https://cvpr.thecvf.com/" rel="external nofollow noopener" target="_blank">CVPR</a></abbr><figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/TeachCLIP-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/TeachCLIP-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/TeachCLIP-1400.webp"></source> <img src="/assets/img/publication_preview/TeachCLIP.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="TeachCLIP.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="TeachCLIP_CVPR2024" class="col-sm-8"> <div class="title">Holistic Features are almost Sufficient for Text-to-Video Retrieval</div> <div class="author"> <a href="https://scholar.google.com/citations?hl=zh-CN&amp;user=p_HrOocAAAAJ" rel="external nofollow noopener" target="_blank">Kaibin Tian*</a>, Ruixiang Zhao*, <a href="https://scholar.google.com/citations?user=wQCmPQkAAAAJ" rel="external nofollow noopener" target="_blank">Zijie Xin</a>, Bangxiang Lan, and <a href="http://lixirong.net/" rel="external nofollow noopener" target="_blank">Xirong Li</a> </div> <div class="periodical"> In <em>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://openaccess.thecvf.com/content/CVPR2024/html/Tian_Holistic_Features_are_almost_Sufficient_for_Text-to-Video_Retrieval_CVPR_2024_paper.html" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Paper</a> <a href="https://github.com/ruc-aimc-lab/TeachCLIP" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://cvpr.thecvf.com/media/PosterPDFs/CVPR%202024/29177.png" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Poster</a> </div> <div class="abstract hidden"> <p>For text-to-video retrieval (T2VR), which aims to retrieve unlabeled videos by ad-hoc textual queries, CLIP-based methods currently lead the way. Compared to CLIP4Clip which is efficient and compact, state-of-the-art models tend to compute video-text similarity through fine-grained cross-modal feature interaction and matching, putting their scalability for large-scale T2VR applications into doubt. We propose TeachCLIP, enabling a CLIP4Clip based student network to learn from more advanced yet computationally intensive models. In order to create a learning channel to convey fine-grained cross-modal knowledge from a heavy model to the student, we add to CLIP4Clip a simple Attentional frame-Feature Aggregation (AFA) block, which by design adds no extra storage / computation overhead at the retrieval stage. Frame-text relevance scores calculated by the teacher network are used as soft labels to supervise the attentive weights produced by AFA. Extensive experiments on multiple public datasets justify the viability of the proposed method. TeachCLIP has the same efficiency and compactness as CLIP4Clip, yet has near-SOTA effectiveness.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr preview"> <abbr class="badge" style="background-color:#89867c"><a href="https://arxiv.org/" rel="external nofollow noopener" target="_blank">arXiv</a></abbr><figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/VMMR-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/VMMR-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/VMMR-1400.webp"></source> <img src="/assets/img/publication_preview/VMMR.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="VMMR.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="VMMR_arxiv" class="col-sm-8"> <div class="title">Video to Music Moment Retrieval</div> <div class="author"> <a href="https://scholar.google.com/citations?user=wQCmPQkAAAAJ" rel="external nofollow noopener" target="_blank">Zijie Xin</a>, Minquan Wang, Ye Ma, Bo Wang, <a href="https://scholar.google.com/citations?user=jFQSmp8AAAAJ" rel="external nofollow noopener" target="_blank">Quan Chen</a>, <a href="https://scholar.google.com/citations?user=9o5swhQAAAAJ" rel="external nofollow noopener" target="_blank">Peng Jiang</a>, and <a href="http://lixirong.net/" rel="external nofollow noopener" target="_blank">Xirong Li</a> </div> <div class="periodical"> <em>Preprint</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/abs/2408.16990" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Paper</a> <a href="https://github.com/xxayt/VMMR" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://rucmm.github.io/VMMR" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Project</a> </div> <div class="abstract hidden"> <p>Adding proper background music helps complete a short video to be shared. Towards automating the task, previous research focuses on video-to-music retrieval (VMR), aiming to find amidst a collection of music the one best matching the content of a given video. Since music tracks are typically much longer than short videos, meaning the returned music has to be cut to a shorter moment, there is a clear gap between the practical need and VMR. In order to bridge the gap, we propose in this paper video to music moment retrieval (VMMR) as a new task. To tackle the new task, we build a comprehensive dataset Ad-Moment which contains 50K short videos annotated with music moments and develop a two-stage approach. In particular, given a test video, the most similar music is retrieved from a given collection. Then, a Transformer based music moment localization is performed. We term this approach Retrieval and Localization (ReaL). Extensive experiments on real-world datasets verify the effectiveness of the proposed method for VMMR.</p> </div> </div> </div> </li> </ol> </div> <div class="social"> <div class="contact-icons"> <a href="mailto:%78%78%61%79%74%33%32%31%36@%67%6D%61%69%6C.%63%6F%6D" title="email"><i class="fas fa-envelope"></i></a> <a href="https://scholar.google.com/citations?user=wQCmPQkAAAAJ" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a> <a href="https://github.com/xxayt" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fab fa-github"></i></a> <a href="https://www.linkedin.com/in/xxayt" title="LinkedIn" rel="external nofollow noopener" target="_blank"><i class="fab fa-linkedin"></i></a> <a href="https://youtube.com/@zijiexin675" title="YouTube" rel="external nofollow noopener" target="_blank"><i class="fab fa-youtube"></i></a> <a id="WeChatBtn" title="WeChat"><i class="fab fa-weixin"></i></a> <div id="WeChatMod" class="wechat-modal"> <img src="/assets/img/wechat-qr.png" alt="WeChat QR" id="WeChatQR"> </div> <script>var wechatModal=document.getElementById("WeChatMod"),wechatBtn=document.getElementById("WeChatBtn");wechatBtn.onclick=function(){wechatModal.style.display="block"},window.onclick=function(t){t.target==wechatModal&&(wechatModal.style.display="none")};</script> </div> <div class="contact-note"> Best way to contact me is to <a href="mailto:xxayt3216@gmail.com">email</a>. </div> </div> <a href="http://s01.flagcounter.com/more/V5" rel="external nofollow noopener" target="_blank"><img src="https://s01.flagcounter.com/mini/V5/bg_FFFFFF/txt_000000/border_CCCCCC/flags_0/" alt="Flag Counter" border="0"> </a> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2024 Zijie Xin. "" </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?c9d9dd48933de3831b3ee5ec9c209cac" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script>var wechatModal=document.getElementById("WeChatMod"),wechatBtn=document.getElementById("WeChatBtn");wechatBtn.onclick=function(){wechatModal.style.display="block"},window.onclick=function(t){t.target==wechatModal&&(wechatModal.style.display="none")};</script> </body> </html>